{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821f2472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimal setup for Llama 3 8B Instruct evaluation\n",
    "import pandas as pd\n",
    "import os\n",
    "import replicate\n",
    "\n",
    "# Load environment variables if needed\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7288df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2996 samples for evaluation\n",
      "Dataset columns: ['index', 'question', 'reference', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5']\n",
      "   index                                           question  \\\n",
      "0      0  Hi doctor,\\nI am a 51-year-old female with a h...   \n",
      "\n",
      "                                           reference  Unnamed: 3  Unnamed: 4  \\\n",
      "0  Hi,\\nWelcome to icliniq.com.\\nSevere jugular v...         NaN         NaN   \n",
      "\n",
      "   Unnamed: 5  \n",
      "0         NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load the entire dataset for full evaluation\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df_sample = df.reset_index(drop=True)\n",
    "print(f\"Using {len(df_sample)} samples for evaluation\")\n",
    "print(f\"Dataset columns: {df_sample.columns.tolist()}\")\n",
    "print(df_sample.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755f8a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Replicate API token loaded from environment\n",
      "Using model: meta/meta-llama-3-8b-instruct\n",
      "✅ Using Llama 3 8B Instruct model from Replicate\n"
     ]
    }
   ],
   "source": [
    "# Setup Replicate API for Llama 3 8B Instruct\n",
    "replicate_api_token = os.getenv('REPLICATE_API_TOKEN')\n",
    "if replicate_api_token:\n",
    "    os.environ['REPLICATE_API_TOKEN'] = replicate_api_token\n",
    "    print(\"[OK] Replicate API token loaded from environment\")\n",
    "else:\n",
    "    print(\"[ERROR] No REPLICATE_API_TOKEN found. Please set it in your .env file\")\n",
    "\n",
    "model_name = \"meta/meta-llama-3-8b-instruct\"  # Llama 3 8B Instruct model\n",
    "print(f\"Using model: {model_name}\")\n",
    "print(\"✅ Using Llama 3 8B Instruct model from Replicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c6ed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Connection successful with meta/meta-llama-3-8b-instruct\n",
      "Test response: Hi! I can help with medical questions.\n"
     ]
    }
   ],
   "source": [
    "# Simple test connection for Llama 3 8B Instruct\n",
    "def test_replicate_connection():\n",
    "    if not replicate_api_token:\n",
    "        print(\"[ERROR] No API token found. Please set REPLICATE_API_TOKEN environment variable.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        test_prompt = \"Hello, please respond with 'Hi! I can help with medical questions.'\"\n",
    "        \n",
    "        output = replicate.run(\n",
    "            model_name,\n",
    "            input={\n",
    "                \"prompt\": test_prompt,\n",
    "                \"max_new_tokens\": 50,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if isinstance(output, list):\n",
    "            response = \"\".join(output)\n",
    "        else:\n",
    "            response = str(output)\n",
    "            \n",
    "        print(f\"[OK] Connection successful with {model_name}\")\n",
    "        print(f\"Test response: {response.strip()}\")\n",
    "        return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Connection error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Test connection\n",
    "api_ready = test_replicate_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c98dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to get response from Llama 3 8B Instruct\n",
    "def get_response(question):\n",
    "    try:\n",
    "        output = replicate.run(\n",
    "            model_name,\n",
    "            input={\n",
    "                \"prompt\": question,\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"temperature\": 0.2\n",
    "            }\n",
    "        )\n",
    "        if isinstance(output, list):\n",
    "            return ''.join(output).strip()\n",
    "        return str(output).strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834f2e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with a medical question:\n",
      "Question: What are the common symptoms of type 2 diabetes?\n",
      "Response: Type 2 diabetes is a chronic condition characterized by high blood sugar levels, and it can develop gradually over time. The common symptoms of type 2 diabetes may not always be noticeable, especially in the early stages. However, some common symptoms include:\n",
      "\n",
      "1. Increased thirst and urination: As the body tries to rid itself of excess glucose, you may feel the need to drink more water and urinate more frequently.\n",
      "2. Fatigue: High blood sugar levels can cause fatigue, weakness, and a general feeling of being tired or sluggish.\n",
      "3. Blurred vision: High blood sugar levels can cause the lens in your eye to swell, leading to blurred vision.\n",
      "4. Slow healing of cuts and wounds: High blood sugar levels can impede the healing process, making cuts and wounds take longer to heal.\n",
      "5. Tingling or numbness in the hands and feet: High blood sugar levels can damage the nerves, causing tingling, numbness, or a burning sensation in the hands and feet.\n",
      "6. Recurring skin, gum, or bladder infections: High blood sugar levels can weaken the immune system, making you more susceptible to infections.\n",
      "7. Cuts or sores that are slow to heal: High blood sugar levels can impede the healing process, making cuts and sores take longer to heal.\n",
      "8. Itching or tingling in the genital area: High blood sugar levels can cause itching or tingling in the genital area.\n",
      "9. Frequent yeast infections: High blood sugar levels can cause yeast infections, especially in women.\n",
      "10. Dark, velvety skin patches (acanthosis nigricans): High blood sugar levels can cause dark, velvety skin patches to appear on the neck, armpits, elbows, knees, and knuckles.\n",
      "\n",
      "It's essential to note that some people with type 2 diabetes may not experience any symptoms at all, especially in the early stages. If you're experiencing any of these symptoms, it's crucial to consult with your healthcare provider to determine the underlying cause and receive proper diagnosis and treatment.\n",
      "\n",
      "In addition to these symptoms, type 2 diabetes can also increase the risk of developing other health complications, such as:\n",
      "\n",
      "* Heart disease and stroke\n",
      "* Kidney disease\n",
      "* Nerve damage (neuropathy)\n",
      "* Eye damage (retinopathy)\n",
      "* Foot damage (ulcers and amputations)\n",
      "* Cognitive impairment and dementia\n",
      "\n",
      "Early detection and management of type 2 diabetes can help prevent or delay these complications and improve overall health outcomes.\n",
      "[OK] Test passed - ready for evaluation!\n",
      "Question: What are the common symptoms of type 2 diabetes?\n",
      "Response: Type 2 diabetes is a chronic condition characterized by high blood sugar levels, and it can develop gradually over time. The common symptoms of type 2 diabetes may not always be noticeable, especially in the early stages. However, some common symptoms include:\n",
      "\n",
      "1. Increased thirst and urination: As the body tries to rid itself of excess glucose, you may feel the need to drink more water and urinate more frequently.\n",
      "2. Fatigue: High blood sugar levels can cause fatigue, weakness, and a general feeling of being tired or sluggish.\n",
      "3. Blurred vision: High blood sugar levels can cause the lens in your eye to swell, leading to blurred vision.\n",
      "4. Slow healing of cuts and wounds: High blood sugar levels can impede the healing process, making cuts and wounds take longer to heal.\n",
      "5. Tingling or numbness in the hands and feet: High blood sugar levels can damage the nerves, causing tingling, numbness, or a burning sensation in the hands and feet.\n",
      "6. Recurring skin, gum, or bladder infections: High blood sugar levels can weaken the immune system, making you more susceptible to infections.\n",
      "7. Cuts or sores that are slow to heal: High blood sugar levels can impede the healing process, making cuts and sores take longer to heal.\n",
      "8. Itching or tingling in the genital area: High blood sugar levels can cause itching or tingling in the genital area.\n",
      "9. Frequent yeast infections: High blood sugar levels can cause yeast infections, especially in women.\n",
      "10. Dark, velvety skin patches (acanthosis nigricans): High blood sugar levels can cause dark, velvety skin patches to appear on the neck, armpits, elbows, knees, and knuckles.\n",
      "\n",
      "It's essential to note that some people with type 2 diabetes may not experience any symptoms at all, especially in the early stages. If you're experiencing any of these symptoms, it's crucial to consult with your healthcare provider to determine the underlying cause and receive proper diagnosis and treatment.\n",
      "\n",
      "In addition to these symptoms, type 2 diabetes can also increase the risk of developing other health complications, such as:\n",
      "\n",
      "* Heart disease and stroke\n",
      "* Kidney disease\n",
      "* Nerve damage (neuropathy)\n",
      "* Eye damage (retinopathy)\n",
      "* Foot damage (ulcers and amputations)\n",
      "* Cognitive impairment and dementia\n",
      "\n",
      "Early detection and management of type 2 diabetes can help prevent or delay these complications and improve overall health outcomes.\n",
      "[OK] Test passed - ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample question\n",
    "if api_ready:\n",
    "    print(\"Testing with a medical question:\")\n",
    "    test_question = \"What are the common symptoms of type 2 diabetes?\"\n",
    "    test_response = get_response(test_question)\n",
    "    print(f\"Question: {test_question}\")\n",
    "    print(f\"Response: {test_response}\")\n",
    "    \n",
    "    if \"Error:\" in test_response:\n",
    "        print(\"[ERROR] Test failed - check API configuration\")\n",
    "    else:\n",
    "        print(\"[OK] Test passed - ready for evaluation!\")\n",
    "else:\n",
    "    print(\"[ERROR] API is not ready. Please check your Replicate API token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62dda936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL DATASET EVALUATION SUMMARY\n",
      "==================================================\n",
      "Total samples to process: 2996\n",
      "Model: meta/meta-llama-3-8b-instruct\n",
      "Output file: llama3_8b_full_evaluation.csv\n",
      "Checkpoint interval: Every 100 samples\n",
      "Estimated time: ~99.9 minutes\n",
      "==================================================\n",
      "[OK] API is ready - starting full evaluation\n"
     ]
    }
   ],
   "source": [
    "# Evaluation summary for full dataset\n",
    "print(\"FULL DATASET EVALUATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples to process: {len(df_sample)}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Output file: llama3_8b_full_evaluation.csv\")\n",
    "print(f\"Checkpoint interval: Every 100 samples\")\n",
    "print(f\"Estimated time: ~{(len(df_sample) * 2) / 60:.1f} minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if api_ready:\n",
    "    print(\"[OK] API is ready - starting full evaluation\")\n",
    "else:\n",
    "    print(\"[ERROR] API not ready. Please check your setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5274154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of 2996 samples using meta/meta-llama-3-8b-instruct...\n",
      "Checkpoints will be saved every 100 samples\n",
      "------------------------------------------------------------\n",
      "Progress: 0/2996 (0.0%) | Rate: 0.0/hour | ETA: 0.0h\n",
      "Failed requests: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m reference = row[\u001b[33m'\u001b[39m\u001b[33mreference\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mreference\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m row \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Get model response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m generated = \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Check for errors\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mError:\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(generated):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mget_response\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(question):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         output = \u001b[43mreplicate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_new_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     13\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(output).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\replicate\\client.py:175\u001b[39m, in \u001b[36mClient.run\u001b[39m\u001b[34m(self, ref, input, use_file_output, **params)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    165\u001b[39m     ref: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    169\u001b[39m     **params: Unpack[\u001b[33m\"\u001b[39m\u001b[33mPredictions.CreatePredictionParams\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    170\u001b[39m ) -> Union[Any, Iterator[Any]]:  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Run a model and wait for its output.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_file_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_file_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\replicate\\run.py:50\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(client, ref, input, use_file_output, **params)\u001b[39m\n\u001b[32m     46\u001b[39m     prediction = client.predictions.create(\n\u001b[32m     47\u001b[39m         version=version_id, \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {}, **params\n\u001b[32m     48\u001b[39m     )\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m owner \u001b[38;5;129;01mand\u001b[39;00m name:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     prediction = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mowner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     55\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected model, version, or reference in the format owner/name or owner/name:version\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\replicate\\model.py:405\u001b[39m, in \u001b[36mModelsPredictions.create\u001b[39m\u001b[34m(self, model, input, **params)\u001b[39m\n\u001b[32m    403\u001b[39m body = _create_prediction_body(version=\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m, **params)\n\u001b[32m    404\u001b[39m extras = _create_prediction_request_params(wait=wait)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _json_to_prediction(\u001b[38;5;28mself\u001b[39m._client, resp.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\replicate\\client.py:88\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, method, path, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, **kwargs) -> httpx.Response:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     _raise_for_status(resp)\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\replicate\\client.py:293\u001b[39m, in \u001b[36mRetryTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: httpx.Request) -> httpx.Response:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_transport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m request.method \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.retryable_methods:\n\u001b[32m    296\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\medical-qa-llm-evaluation\\.conda\\Lib\\ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run evaluation on full dataset with checkpoints\n",
    "import time\n",
    "\n",
    "results = []\n",
    "start_time = time.time()\n",
    "total_samples = len(df_sample)\n",
    "failed_requests = 0\n",
    "\n",
    "print(f\"Starting evaluation of {total_samples} samples using {model_name}...\")\n",
    "print(f\"Checkpoints will be saved every 100 samples\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, row in df_sample.iterrows():\n",
    "    # Progress updates every 50 samples\n",
    "    if i % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i / elapsed * 3600) if elapsed > 0 else 0\n",
    "        eta_hours = (total_samples - i) / rate if rate > 0 else 0\n",
    "        completed_pct = (i / total_samples) * 100\n",
    "        print(f\"Progress: {i}/{total_samples} ({completed_pct:.1f}%) | Rate: {rate:.1f}/hour | ETA: {eta_hours:.1f}h\")\n",
    "        print(f\"Failed requests: {failed_requests}\")\n",
    "    \n",
    "    question = row['question']\n",
    "    reference = row['reference'] if 'reference' in row else ''\n",
    "    \n",
    "    # Get model response\n",
    "    generated = get_response(question)\n",
    "    \n",
    "    # Check for errors\n",
    "    if \"Error:\" in str(generated):\n",
    "        failed_requests += 1\n",
    "        print(f\"[WARNING] Failed request at sample {i}\")\n",
    "        continue\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'question': question,\n",
    "        'reference': reference,\n",
    "        'generated': generated\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint every 100 samples\n",
    "    if (i + 1) % 100 == 0:\n",
    "        temp_df = pd.DataFrame(results)\n",
    "        checkpoint_file = f'checkpoint_{i+1}.csv'\n",
    "        temp_df.to_csv(checkpoint_file, index=False)\n",
    "        print(f\"[CHECKPOINT] Saved {len(results)} results to {checkpoint_file}\")\n",
    "    \n",
    "    # Rate limiting - 1 second delay to avoid hitting API limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Final save\n",
    "if len(results) > 0:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('llama3_8b_full_evaluation.csv', index=False)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION COMPLETED!\")\n",
    "    print(f\"Successfully processed: {len(results_df)} samples\")\n",
    "    print(f\"Failed requests: {failed_requests}\")\n",
    "    print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "    print(f\"Results saved to 'llama3_8b_full_evaluation.csv'\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\n[ERROR] NO VALID RESULTS - Evaluation failed\")\n",
    "    print(\"Please check the model configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b5cba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing results up to index: 1427\n",
      "Total existing samples: 1427\n",
      "Loaded 1427 existing results\n",
      "\n",
      "Resuming evaluation from index 1428\n",
      "Remaining samples to process: 1568\n",
      "Progress: 1428/2996 (47.7% completed)\n",
      "------------------------------------------------------------\n",
      "Progress: 1450/2996 (48.4%) | Rate: 514.6/hour | ETA: 3.0h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1499 results to checkpoint_1500.csv\n",
      "Progress: 1500/2996 (50.1%) | Rate: 500.5/hour | ETA: 3.0h\n",
      "Failed requests: 0\n",
      "Progress: 1550/2996 (51.7%) | Rate: 510.1/hour | ETA: 2.8h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1599 results to checkpoint_1600.csv\n",
      "Progress: 1600/2996 (53.4%) | Rate: 515.2/hour | ETA: 2.7h\n",
      "Failed requests: 0\n",
      "Progress: 1650/2996 (55.1%) | Rate: 516.6/hour | ETA: 2.6h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1699 results to checkpoint_1700.csv\n",
      "Progress: 1700/2996 (56.7%) | Rate: 507.9/hour | ETA: 2.6h\n",
      "Failed requests: 0\n",
      "Progress: 1750/2996 (58.4%) | Rate: 509.0/hour | ETA: 2.4h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1799 results to checkpoint_1800.csv\n",
      "Progress: 1800/2996 (60.1%) | Rate: 511.2/hour | ETA: 2.3h\n",
      "Failed requests: 0\n",
      "Progress: 1850/2996 (61.7%) | Rate: 509.4/hour | ETA: 2.2h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1899 results to checkpoint_1900.csv\n",
      "Progress: 1900/2996 (63.4%) | Rate: 508.4/hour | ETA: 2.2h\n",
      "Failed requests: 0\n",
      "Progress: 1950/2996 (65.1%) | Rate: 506.3/hour | ETA: 2.1h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1999 results to checkpoint_2000.csv\n",
      "Progress: 2000/2996 (66.8%) | Rate: 505.9/hour | ETA: 2.0h\n",
      "Failed requests: 0\n",
      "[WARNING] Failed request at sample 2006\n",
      "[WARNING] Failed request at sample 2007\n",
      "Progress: 2050/2996 (68.4%) | Rate: 504.2/hour | ETA: 1.9h\n",
      "Failed requests: 2\n",
      "[CHECKPOINT] Saved 2097 results to checkpoint_2100.csv\n",
      "Progress: 2100/2996 (70.1%) | Rate: 505.6/hour | ETA: 1.8h\n",
      "Failed requests: 2\n",
      "Progress: 2150/2996 (71.8%) | Rate: 505.1/hour | ETA: 1.7h\n",
      "Failed requests: 2\n",
      "[CHECKPOINT] Saved 2197 results to checkpoint_2200.csv\n",
      "Progress: 2200/2996 (73.4%) | Rate: 504.5/hour | ETA: 1.6h\n",
      "Failed requests: 2\n",
      "Progress: 2250/2996 (75.1%) | Rate: 504.5/hour | ETA: 1.5h\n",
      "Failed requests: 2\n",
      "[CHECKPOINT] Saved 2297 results to checkpoint_2300.csv\n",
      "Progress: 2300/2996 (76.8%) | Rate: 502.3/hour | ETA: 1.4h\n",
      "Failed requests: 2\n",
      "Progress: 2350/2996 (78.4%) | Rate: 503.2/hour | ETA: 1.3h\n",
      "Failed requests: 2\n",
      "[CHECKPOINT] Saved 2397 results to checkpoint_2400.csv\n",
      "Progress: 2400/2996 (80.1%) | Rate: 500.8/hour | ETA: 1.2h\n",
      "Failed requests: 2\n",
      "Progress: 2450/2996 (81.8%) | Rate: 500.3/hour | ETA: 1.1h\n",
      "Failed requests: 2\n",
      "[CHECKPOINT] Saved 2497 results to checkpoint_2500.csv\n",
      "Progress: 2500/2996 (83.4%) | Rate: 499.4/hour | ETA: 1.0h\n",
      "Failed requests: 2\n",
      "Progress: 2550/2996 (85.1%) | Rate: 498.5/hour | ETA: 0.9h\n",
      "Failed requests: 2\n",
      "[CHECKPOINT] Saved 2597 results to checkpoint_2600.csv\n",
      "Progress: 2600/2996 (86.8%) | Rate: 499.2/hour | ETA: 0.8h\n",
      "Failed requests: 2\n",
      "[WARNING] Failed request at sample 2622\n",
      "Progress: 2650/2996 (88.5%) | Rate: 476.6/hour | ETA: 0.7h\n",
      "Failed requests: 3\n",
      "[CHECKPOINT] Saved 2696 results to checkpoint_2700.csv\n",
      "Progress: 2700/2996 (90.1%) | Rate: 477.2/hour | ETA: 0.6h\n",
      "Failed requests: 3\n",
      "Progress: 2750/2996 (91.8%) | Rate: 476.5/hour | ETA: 0.5h\n",
      "Failed requests: 3\n",
      "[CHECKPOINT] Saved 2796 results to checkpoint_2800.csv\n",
      "Progress: 2800/2996 (93.5%) | Rate: 475.2/hour | ETA: 0.4h\n",
      "Failed requests: 3\n",
      "Progress: 2850/2996 (95.1%) | Rate: 475.7/hour | ETA: 0.3h\n",
      "Failed requests: 3\n",
      "[CHECKPOINT] Saved 2896 results to checkpoint_2900.csv\n",
      "Progress: 2900/2996 (96.8%) | Rate: 475.8/hour | ETA: 0.2h\n",
      "Failed requests: 3\n",
      "Progress: 2950/2996 (98.5%) | Rate: 477.2/hour | ETA: 0.1h\n",
      "Failed requests: 3\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETED!\n",
      "Successfully processed: 2992 samples\n",
      "Failed requests: 3\n",
      "Total time for this session: 3.28 hours\n",
      "Results saved to 'llama3_8b_full_evaluation.csv'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Resume evaluation from the last checkpoint\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Load existing results to determine where to resume\n",
    "try:\n",
    "    existing_results = pd.read_csv('llama3_8b_full_evaluation.csv')\n",
    "    last_processed_index = existing_results['index'].max()\n",
    "    print(f\"Found existing results up to index: {last_processed_index}\")\n",
    "    print(f\"Total existing samples: {len(existing_results)}\")\n",
    "    \n",
    "    # Convert existing results to list format for continuation\n",
    "    results = existing_results.to_dict('records')\n",
    "    print(f\"Loaded {len(results)} existing results\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"No existing results file found. Starting fresh.\")\n",
    "    results = []\n",
    "    last_processed_index = -1\n",
    "\n",
    "# Resume from the next index\n",
    "start_index = last_processed_index + 1\n",
    "total_samples = len(df_sample)\n",
    "failed_requests = 0\n",
    "\n",
    "print(f\"\\nResuming evaluation from index {start_index}\")\n",
    "print(f\"Remaining samples to process: {total_samples - start_index}\")\n",
    "print(f\"Progress: {start_index}/{total_samples} ({(start_index/total_samples)*100:.1f}% completed)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Resume evaluation loop\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(start_index, total_samples):\n",
    "    row = df_sample.iloc[i]\n",
    "    \n",
    "    # Progress updates every 50 samples\n",
    "    if i % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = ((i - start_index) / elapsed * 3600) if elapsed > 0 else 0\n",
    "        eta_hours = (total_samples - i) / rate if rate > 0 else 0\n",
    "        completed_pct = (i / total_samples) * 100\n",
    "        print(f\"Progress: {i}/{total_samples} ({completed_pct:.1f}%) | Rate: {rate:.1f}/hour | ETA: {eta_hours:.1f}h\")\n",
    "        print(f\"Failed requests: {failed_requests}\")\n",
    "    \n",
    "    question = row['question']\n",
    "    reference = row['reference'] if 'reference' in row else ''\n",
    "    \n",
    "    # Get model response\n",
    "    generated = get_response(question)\n",
    "    \n",
    "    # Check for errors\n",
    "    if \"Error:\" in str(generated):\n",
    "        failed_requests += 1\n",
    "        print(f\"[WARNING] Failed request at sample {i}\")\n",
    "        continue\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'question': question,\n",
    "        'reference': reference,\n",
    "        'generated': generated\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint every 100 samples\n",
    "    if (i + 1) % 100 == 0:\n",
    "        temp_df = pd.DataFrame(results)\n",
    "        checkpoint_file = f'checkpoint_{i+1}.csv'\n",
    "        temp_df.to_csv(checkpoint_file, index=False)\n",
    "        print(f\"[CHECKPOINT] Saved {len(results)} results to {checkpoint_file}\")\n",
    "    \n",
    "    # Rate limiting - 1 second delay to avoid hitting API limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Final save\n",
    "if len(results) > 0:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('llama3_8b_full_evaluation.csv', index=False)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION COMPLETED!\")\n",
    "    print(f\"Successfully processed: {len(results_df)} samples\")\n",
    "    print(f\"Failed requests: {failed_requests}\")\n",
    "    print(f\"Total time for this session: {total_time/3600:.2f} hours\")\n",
    "    print(f\"Results saved to 'llama3_8b_full_evaluation.csv'\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\n[ERROR] NO VALID RESULTS - Evaluation failed\")\n",
    "    print(\"Please check the model configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97808ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL EVALUATION RESULTS ===\n",
      "Total samples evaluated: 2992\n",
      "Model: meta/meta-llama-3-8b-instruct\n",
      "\n",
      "Sample results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "Question: Hi doctor,\n",
      "I am a 51-year-old female with a height of 5 feet 8 inches and a weight of 145 lbs. I hav...\n",
      "Generated: I'm not a doctor, but I can try to help you understand your situation and provide some general infor...\n",
      "\n",
      "--- Sample 2 ---\n",
      "Question: Hello doctor,\n",
      "I am 29 years old.\n",
      "CT scan came up negative for problems associated with infection. I ...\n",
      "Generated: Thank you for reaching out and sharing your concerns with me. I'm here to help you get a better unde...\n",
      "\n",
      "--- Sample 3 ---\n",
      "Question: Hi doctor,\n",
      "For the past two days, I am suffering from a cold because of climatic change. I am having...\n",
      "Generated: Sorry to hear that you're not feeling well! It's not uncommon to catch a cold due to climatic change...\n",
      "\n",
      "--- Sample 4 ---\n",
      "Question: Hi doctor,\n",
      "I am suffering from kidney stones problem for the past four days.  I took an ultrasound s...\n",
      "Generated: I'm not a doctor, but I can try to help you understand your situation and provide some guidance. How...\n",
      "\n",
      "--- Sample 5 ---\n",
      "Question: Hi doctor,\n",
      "I am a 20-year-old female. I am having early morning blood in saliva for the past two mon...\n",
      "Generated: Thank you for reaching out and sharing your concerns with me. I'm here to help you explore possible ...\n",
      "\n",
      "Full results saved to 'llama3_8b_full_evaluation.csv'\n",
      "Dataset size: 2992 samples\n",
      "Average response length: 2012.4 characters\n"
     ]
    }
   ],
   "source": [
    "# Display full evaluation results\n",
    "try:\n",
    "    results_df = pd.read_csv('llama3_8b_full_evaluation.csv')\n",
    "    \n",
    "    print(\"=== FULL EVALUATION RESULTS ===\")\n",
    "    print(f\"Total samples evaluated: {len(results_df)}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    print(\"\\nSample results:\")\n",
    "    for i in range(min(5, len(results_df))):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"Question: {results_df.iloc[i]['question'][:100]}...\")\n",
    "        print(f\"Generated: {results_df.iloc[i]['generated'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nFull results saved to 'llama3_8b_full_evaluation.csv'\")\n",
    "    print(f\"Dataset size: {len(results_df)} samples\")\n",
    "    \n",
    "    # Show statistics\n",
    "    avg_response_length = results_df['generated'].str.len().mean()\n",
    "    print(f\"Average response length: {avg_response_length:.1f} characters\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"[ERROR] No evaluation results found. Please run the evaluation first.\")\n",
    "    print(\"Looking for: llama3_8b_full_evaluation.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error loading results: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e82bbcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import evaluation metrics libraries\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Setup metrics\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smoothing = SmoothingFunction().method1\n",
    "\n",
    "print(\"Evaluation metrics libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f01f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics calculation function ready\n"
     ]
    }
   ],
   "source": [
    "# Define metrics calculation function\n",
    "def calculate_metrics(reference, generated):\n",
    "    \"\"\"Calculate BLEU and ROUGE metrics for a reference-generated pair\"\"\"\n",
    "    try:\n",
    "        # Tokenize for BLEU calculation\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        gen_tokens = nltk.word_tokenize(generated.lower())\n",
    "        \n",
    "        # Calculate BLEU scores\n",
    "        bleu1 = sentence_bleu([ref_tokens], gen_tokens, weights=(1,0,0,0), smoothing_function=smoothing)\n",
    "        bleu4 = sentence_bleu([ref_tokens], gen_tokens, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothing)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer_obj.score(reference, generated)\n",
    "        \n",
    "        return {\n",
    "            'bleu1': bleu1,\n",
    "            'bleu4': bleu4,\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics for sample: {e}\")\n",
    "        return {\n",
    "            'bleu1': 0.0,\n",
    "            'bleu4': 0.0,\n",
    "            'rouge1': 0.0,\n",
    "            'rouge2': 0.0,\n",
    "            'rougeL': 0.0\n",
    "        }\n",
    "\n",
    "print(\"Metrics calculation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2c199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded evaluation results with 2992 samples\n",
      "Calculating metrics for each sample...\n",
      "Processing sample 0/2992\n",
      "Processing sample 100/2992\n",
      "Processing sample 100/2992\n",
      "Processing sample 200/2992\n",
      "Processing sample 200/2992\n",
      "Processing sample 300/2992\n",
      "Processing sample 300/2992\n",
      "Processing sample 400/2992\n",
      "Processing sample 400/2992\n",
      "Processing sample 500/2992\n",
      "Processing sample 500/2992\n",
      "Processing sample 600/2992\n",
      "Processing sample 600/2992\n",
      "Processing sample 700/2992\n",
      "Processing sample 700/2992\n",
      "Processing sample 800/2992\n",
      "Processing sample 800/2992\n",
      "Processing sample 900/2992\n",
      "Processing sample 900/2992\n",
      "Processing sample 1000/2992\n",
      "Processing sample 1000/2992\n",
      "Processing sample 1100/2992\n",
      "Processing sample 1100/2992\n",
      "Processing sample 1200/2992\n",
      "Processing sample 1200/2992\n",
      "Processing sample 1300/2992\n",
      "Processing sample 1300/2992\n",
      "Processing sample 1400/2992\n",
      "Processing sample 1400/2992\n",
      "Processing sample 1500/2992\n",
      "Processing sample 1500/2992\n",
      "Processing sample 1600/2992\n",
      "Processing sample 1600/2992\n",
      "Processing sample 1700/2992\n",
      "Processing sample 1700/2992\n",
      "Processing sample 1800/2992\n",
      "Processing sample 1800/2992\n",
      "Processing sample 1900/2992\n",
      "Processing sample 1900/2992\n",
      "Processing sample 2000/2992\n",
      "Processing sample 2000/2992\n",
      "Processing sample 2100/2992\n",
      "Processing sample 2100/2992\n",
      "Processing sample 2200/2992\n",
      "Processing sample 2200/2992\n",
      "Processing sample 2300/2992\n",
      "Processing sample 2300/2992\n",
      "Processing sample 2400/2992\n",
      "Processing sample 2400/2992\n",
      "Processing sample 2500/2992\n",
      "Processing sample 2500/2992\n",
      "Processing sample 2600/2992\n",
      "Processing sample 2600/2992\n",
      "Processing sample 2700/2992\n",
      "Processing sample 2700/2992\n",
      "Processing sample 2800/2992\n",
      "Processing sample 2800/2992\n",
      "Processing sample 2900/2992\n",
      "Processing sample 2900/2992\n",
      "\n",
      "Metrics calculated for 2992 samples\n",
      "Updated results saved to 'llama3_8b_full_evaluation_with_metrics.csv'\n",
      "\n",
      "Metrics calculated for 2992 samples\n",
      "Updated results saved to 'llama3_8b_full_evaluation_with_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for each sample in the evaluation results\n",
    "try:\n",
    "    # Load the evaluation results\n",
    "    results_df = pd.read_csv('llama3_8b_full_evaluation.csv')\n",
    "    print(f\"Loaded evaluation results with {len(results_df)} samples\")\n",
    "    \n",
    "    # Check if metrics columns already exist\n",
    "    metric_columns = ['bleu1', 'bleu4', 'rouge1', 'rouge2', 'rougeL']\n",
    "    missing_metrics = [col for col in metric_columns if col not in results_df.columns]\n",
    "    \n",
    "    if missing_metrics:\n",
    "        print(\"Calculating metrics for each sample...\")\n",
    "        \n",
    "        # Add metrics columns\n",
    "        for metric in metric_columns:\n",
    "            results_df[metric] = None\n",
    "        \n",
    "        # Calculate metrics for each sample\n",
    "        valid_metrics_count = 0\n",
    "        for i, row in results_df.iterrows():\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processing sample {i}/{len(results_df)}\")\n",
    "            \n",
    "            reference = row['reference'] if pd.notna(row['reference']) else ''\n",
    "            generated = row['generated'] if pd.notna(row['generated']) else ''\n",
    "            \n",
    "            # Only calculate metrics if both reference and generated text exist\n",
    "            if reference and reference.strip() and generated and generated.strip():\n",
    "                metrics = calculate_metrics(reference, generated)\n",
    "                for metric, value in metrics.items():\n",
    "                    results_df.at[i, metric] = value\n",
    "                valid_metrics_count += 1\n",
    "        \n",
    "        # Save updated results with metrics\n",
    "        results_df.to_csv('llama3_8b_full_evaluation_with_metrics.csv', index=False)\n",
    "        print(f\"\\nMetrics calculated for {valid_metrics_count} samples\")\n",
    "        print(\"Updated results saved to 'llama3_8b_full_evaluation_with_metrics.csv'\")\n",
    "    else:\n",
    "        print(\"Metrics columns already exist in the dataset\")\n",
    "        valid_metrics_count = results_df['bleu1'].notna().sum()\n",
    "        print(f\"Found metrics for {valid_metrics_count} samples\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No evaluation results found.\")\n",
    "    print(\"Please run the evaluation first to generate 'llama3_8b_full_evaluation.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating metrics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07763786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded results with calculated metrics\n",
      "\n",
      "============================================================\n",
      "EVALUATION METRICS ANALYSIS\n",
      "============================================================\n",
      "Total samples: 2992\n",
      "Model: meta/meta-llama-3-8b-instruct\n",
      "\n",
      "=== AVERAGE METRICS (5 metrics available) ===\n",
      "    BLEU-1: 0.1739 (±0.0926) | Range: [0.0000, 0.4878] | 2992 samples\n",
      "    BLEU-4: 0.0127 (±0.0147) | Range: [0.0000, 0.1400] | 2992 samples\n",
      "   ROUGE-1: 0.2419 (±0.0896) | Range: [0.0000, 0.5686] | 2992 samples\n",
      "   ROUGE-2: 0.0379 (±0.0270) | Range: [0.0000, 0.2289] | 2992 samples\n",
      "   ROUGE-L: 0.1219 (±0.0351) | Range: [0.0000, 0.2793] | 2992 samples\n",
      "\n",
      "=== SCORE DISTRIBUTION (QUARTILES) ===\n",
      "    BLEU-1: Q25=0.1163 | Median=0.1701 | Q75=0.2301\n",
      "    BLEU-4: Q25=0.0040 | Median=0.0082 | Q75=0.0162\n",
      "   ROUGE-1: Q25=0.1831 | Median=0.2435 | Q75=0.3025\n",
      "   ROUGE-2: Q25=0.0202 | Median=0.0340 | Q75=0.0499\n",
      "   ROUGE-L: Q25=0.1017 | Median=0.1243 | Q75=0.1420\n",
      "\n",
      "=== SAMPLE ANALYSIS ===\n",
      "\n",
      "Best performing sample (highest BLEU-1: 0.4878):\n",
      "Question: Hello doctor,I am a 65-year-old woman concerned about my overall respiratory health, particularly re...\n",
      "Reference: Hi,\n",
      "Welcome to icliniq.com.\n",
      "I understand your questions about health at this age.\n",
      "Certainly, as we a...\n",
      "Generated: As a 65-year-old woman, you're wise to prioritize your respiratory health, especially during flu sea...\n",
      "\n",
      "Worst performing sample (lowest BLEU-1: 0.0000):\n",
      "Question: Hello doctor,I am a 23-year-old male, and I have been dealing with premature ejaculation issues. I u...\n",
      "Reference: Hello,\n",
      "Welcome to icliniq.com.\n",
      "I read your query.\n",
      "I understand your concerns regarding premature eja...\n",
      "Generated: I'm not a doctor, so I can't provide medical advice....\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Samples with calculated metrics: 2992\n",
      "Samples with reference text: 2992\n",
      "Average response length: 2012.4 characters\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display average metrics\n",
    "try:\n",
    "    # Load results with metrics\n",
    "    try:\n",
    "        results_df = pd.read_csv('llama3_8b_full_evaluation_with_metrics.csv')\n",
    "        print(\"Loaded results with calculated metrics\")\n",
    "    except FileNotFoundError:\n",
    "        results_df = pd.read_csv('llama3_8b_full_evaluation.csv')\n",
    "        print(\"Loaded original results (metrics may not be available)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION METRICS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total samples: {len(results_df)}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    # Check which metrics are available\n",
    "    metric_columns = ['bleu1', 'bleu4', 'rouge1', 'rouge2', 'rougeL']\n",
    "    metric_names = ['BLEU-1', 'BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "    \n",
    "    available_metrics = [col for col in metric_columns if col in results_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        print(f\"\\n=== AVERAGE METRICS ({len(available_metrics)} metrics available) ===\")\n",
    "        \n",
    "        metrics_summary = {}\n",
    "        for metric, name in zip(metric_columns, metric_names):\n",
    "            if metric in results_df.columns:\n",
    "                # Filter out None/NaN values\n",
    "                valid_scores = results_df[metric].dropna()\n",
    "                if len(valid_scores) > 0:\n",
    "                    avg_score = valid_scores.mean()\n",
    "                    std_score = valid_scores.std()\n",
    "                    min_score = valid_scores.min()\n",
    "                    max_score = valid_scores.max()\n",
    "                    \n",
    "                    metrics_summary[metric] = {\n",
    "                        'avg': avg_score, \n",
    "                        'std': std_score, \n",
    "                        'min': min_score, \n",
    "                        'max': max_score,\n",
    "                        'count': len(valid_scores)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"{name:>10}: {avg_score:.4f} (±{std_score:.4f}) | Range: [{min_score:.4f}, {max_score:.4f}] | {len(valid_scores)} samples\")\n",
    "                else:\n",
    "                    print(f\"{name:>10}: No valid scores\")\n",
    "        \n",
    "        # Show quartile distribution\n",
    "        if metrics_summary:\n",
    "            print(f\"\\n=== SCORE DISTRIBUTION (QUARTILES) ===\")\n",
    "            for metric, name in zip(metric_columns, metric_names):\n",
    "                if metric in metrics_summary:\n",
    "                    valid_scores = results_df[metric].dropna()\n",
    "                    q25 = valid_scores.quantile(0.25)\n",
    "                    q50 = valid_scores.quantile(0.50)  # median\n",
    "                    q75 = valid_scores.quantile(0.75)\n",
    "                    print(f\"{name:>10}: Q25={q25:.4f} | Median={q50:.4f} | Q75={q75:.4f}\")\n",
    "        \n",
    "        # Show best and worst performing samples\n",
    "        if 'bleu1' in results_df.columns:\n",
    "            print(f\"\\n=== SAMPLE ANALYSIS ===\")\n",
    "            valid_samples = results_df.dropna(subset=['bleu1'])\n",
    "            \n",
    "            if len(valid_samples) > 0:\n",
    "                # Best performing sample (highest BLEU-1)\n",
    "                best_idx = valid_samples['bleu1'].idxmax()\n",
    "                best_sample = results_df.loc[best_idx]\n",
    "                print(f\"\\nBest performing sample (highest BLEU-1: {best_sample['bleu1']:.4f}):\")\n",
    "                print(f\"Question: {best_sample['question'][:100]}...\")\n",
    "                print(f\"Reference: {best_sample['reference'][:100]}...\")\n",
    "                print(f\"Generated: {best_sample['generated'][:100]}...\")\n",
    "                \n",
    "                # Worst performing sample (lowest BLEU-1)\n",
    "                worst_idx = valid_samples['bleu1'].idxmin()\n",
    "                worst_sample = results_df.loc[worst_idx]\n",
    "                print(f\"\\nWorst performing sample (lowest BLEU-1: {worst_sample['bleu1']:.4f}):\")\n",
    "                print(f\"Question: {worst_sample['question'][:100]}...\")\n",
    "                print(f\"Reference: {worst_sample['reference'][:100]}...\")\n",
    "                print(f\"Generated: {worst_sample['generated'][:100]}...\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n=== SUMMARY STATISTICS ===\")\n",
    "        samples_with_metrics = results_df[available_metrics[0]].notna().sum()\n",
    "        samples_with_reference = results_df['reference'].notna().sum() if 'reference' in results_df.columns else 0\n",
    "        avg_response_length = results_df['generated'].str.len().mean()\n",
    "        \n",
    "        print(f\"Samples with calculated metrics: {samples_with_metrics}\")\n",
    "        print(f\"Samples with reference text: {samples_with_reference}\")\n",
    "        print(f\"Average response length: {avg_response_length:.1f} characters\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nNo metrics available. Please run the metrics calculation first.\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No evaluation results found.\")\n",
    "    print(\"Please run the evaluation and metrics calculation first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing metrics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export metrics summary to CSV\n",
    "try:\n",
    "    # Load results with metrics\n",
    "    try:\n",
    "        results_df = pd.read_csv('llama3_8b_full_evaluation_with_metrics.csv')\n",
    "    except FileNotFoundError:\n",
    "        results_df = pd.read_csv('llama3_8b_full_evaluation.csv')\n",
    "    \n",
    "    # Create comprehensive metrics summary\n",
    "    metric_columns = ['bleu1', 'bleu4', 'rouge1', 'rouge2', 'rougeL']\n",
    "    summary_data = []\n",
    "    \n",
    "    print(\"Creating metrics summary report...\")\n",
    "    \n",
    "    for metric in metric_columns:\n",
    "        if metric in results_df.columns:\n",
    "            valid_scores = results_df[metric].dropna()\n",
    "            if len(valid_scores) > 0:\n",
    "                summary_data.append({\n",
    "                    'metric': metric.upper(),\n",
    "                    'mean': valid_scores.mean(),\n",
    "                    'std': valid_scores.std(),\n",
    "                    'min': valid_scores.min(),\n",
    "                    'q25': valid_scores.quantile(0.25),\n",
    "                    'median': valid_scores.quantile(0.50),\n",
    "                    'q75': valid_scores.quantile(0.75),\n",
    "                    'max': valid_scores.max(),\n",
    "                    'sample_count': len(valid_scores),\n",
    "                    'model': model_name,\n",
    "                    'total_samples': len(results_df)\n",
    "                })\n",
    "    \n",
    "    if summary_data:\n",
    "        # Create and save summary DataFrame\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv('llama3_8b_metrics_summary.csv', index=False)\n",
    "        \n",
    "        print(\"Metrics summary exported to 'llama3_8b_metrics_summary.csv'\")\n",
    "        print(\"\\nMETRICS SUMMARY TABLE:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display formatted summary\n",
    "        for _, row in summary_df.iterrows():\n",
    "            print(f\"{row['metric']:>8}: {row['mean']:.4f} ± {row['std']:.4f} \"\n",
    "                  f\"(range: {row['min']:.4f}-{row['max']:.4f}) | {row['sample_count']} samples\")\n",
    "        \n",
    "        # Calculate overall performance score (weighted average)\n",
    "        if len(summary_df) >= 3:  # If we have at least 3 metrics\n",
    "            weights = {'BLEU1': 0.2, 'BLEU4': 0.2, 'ROUGE1': 0.2, 'ROUGE2': 0.2, 'ROUGEL': 0.2}\n",
    "            total_score = 0\n",
    "            total_weight = 0\n",
    "            \n",
    "            for _, row in summary_df.iterrows():\n",
    "                metric_name = row['metric']\n",
    "                if metric_name in weights:\n",
    "                    total_score += row['mean'] * weights[metric_name]\n",
    "                    total_weight += weights[metric_name]\n",
    "            \n",
    "            if total_weight > 0:\n",
    "                overall_score = total_score / total_weight\n",
    "                print(f\"\\nOVERALL PERFORMANCE SCORE: {overall_score:.4f}\")\n",
    "                \n",
    "                # Performance interpretation\n",
    "                if overall_score >= 0.4:\n",
    "                    print(\"INTERPRETATION: EXCELLENT performance\")\n",
    "                elif overall_score >= 0.3:\n",
    "                    print(\"INTERPRETATION: GOOD performance\") \n",
    "                elif overall_score >= 0.2:\n",
    "                    print(\"INTERPRETATION: MODERATE performance\")\n",
    "                else:\n",
    "                    print(\"INTERPRETATION: NEEDS IMPROVEMENT\")\n",
    "        \n",
    "        print(f\"\\nFiles created:\")\n",
    "        print(f\"- llama3_8b_full_evaluation_with_metrics.csv (detailed results)\")\n",
    "        print(f\"- llama3_8b_metrics_summary.csv (summary statistics)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No metrics data available to export\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating metrics summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Metrics Summary - Calculate 5 metrics for all samples and show averages\n",
    "try:\n",
    "    # Load the final results\n",
    "    results_df = pd.read_csv('llama3_8b_full_evaluation_with_metrics.csv')\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"FINAL EVALUATION SUMMARY - LLAMA 3 8B INSTRUCT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Total samples processed: {len(results_df)}\")\n",
    "    \n",
    "    # Calculate metrics for all samples\n",
    "    metric_columns = ['bleu1', 'bleu4', 'rouge1', 'rouge2', 'rougeL']\n",
    "    metric_names = ['BLEU-1', 'BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "    \n",
    "    print(f\"\\n{'METRIC':<12} {'AVERAGE':<10} {'SAMPLES':<8} {'MIN':<8} {'MAX':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    final_metrics = {}\n",
    "    for metric, name in zip(metric_columns, metric_names):\n",
    "        if metric in results_df.columns:\n",
    "            valid_scores = results_df[metric].dropna()\n",
    "            if len(valid_scores) > 0:\n",
    "                avg = valid_scores.mean()\n",
    "                min_val = valid_scores.min()\n",
    "                max_val = valid_scores.max()\n",
    "                count = len(valid_scores)\n",
    "                \n",
    "                final_metrics[name] = avg\n",
    "                print(f\"{name:<12} {avg:<10.4f} {count:<8} {min_val:<8.4f} {max_val:<8.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"Files saved:\")\n",
    "    print(\"- llama3_8b_full_evaluation.csv (raw results)\")\n",
    "    print(\"- llama3_8b_full_evaluation_with_metrics.csv (with metrics)\")\n",
    "    print(\"- llama3_8b_metrics_summary.csv (summary report)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Evaluation files not found. Please run the complete evaluation pipeline first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in final summary: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
