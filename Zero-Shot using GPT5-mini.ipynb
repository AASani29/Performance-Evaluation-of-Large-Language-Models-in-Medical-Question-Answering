{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "821f2472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimal setup for GPT-5-mini evaluation\n",
    "import pandas as pd\n",
    "import os\n",
    "import replicate\n",
    "\n",
    "# Load environment variables if needed\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7288df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2996 samples for evaluation\n",
      "Dataset columns: ['index', 'question', 'reference', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5']\n",
      "   index                                           question  \\\n",
      "0      0  Hi doctor,\\nI am a 51-year-old female with a h...   \n",
      "\n",
      "                                           reference  Unnamed: 3  Unnamed: 4  \\\n",
      "0  Hi,\\nWelcome to icliniq.com.\\nSevere jugular v...         NaN         NaN   \n",
      "\n",
      "   Unnamed: 5  \n",
      "0         NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load the entire dataset for full evaluation\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df_sample = df.reset_index(drop=True)\n",
    "print(f\"Using {len(df_sample)} samples for evaluation\")\n",
    "print(f\"Dataset columns: {df_sample.columns.tolist()}\")\n",
    "print(df_sample.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "755f8a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Replicate API token loaded from environment\n",
      "Using model: openai/gpt-5-mini\n",
      "✅ Using verified GPT-5-mini model from Replicate\n"
     ]
    }
   ],
   "source": [
    "# Setup Replicate API for GPT-5-mini\n",
    "replicate_api_token = os.getenv('REPLICATE_API_TOKEN')\n",
    "if replicate_api_token:\n",
    "    os.environ['REPLICATE_API_TOKEN'] = replicate_api_token\n",
    "    print(\"[OK] Replicate API token loaded from environment\")\n",
    "else:\n",
    "    print(\"[ERROR] No REPLICATE_API_TOKEN found. Please set it in your .env file\")\n",
    "\n",
    "model_name = \"openai/gpt-5-mini\"  # Correct Replicate model path\n",
    "print(f\"Using model: {model_name}\")\n",
    "print(\"✅ Using verified GPT-5-mini model from Replicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1c6ed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Connection successful with openai/gpt-5-mini\n",
      "Test response: Hi! I can help with medical questions.\n"
     ]
    }
   ],
   "source": [
    "# Simple test connection for GPT-5-mini\n",
    "def test_replicate_connection():\n",
    "    if not replicate_api_token:\n",
    "        print(\"[ERROR] No API token found. Please set REPLICATE_API_TOKEN environment variable.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        test_prompt = \"Hello, please respond with 'Hi! I can help with medical questions.'\"\n",
    "        \n",
    "        output = replicate.run(\n",
    "            model_name,\n",
    "            input={\n",
    "                \"prompt\": test_prompt,\n",
    "                \"max_tokens\": 50,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if isinstance(output, list):\n",
    "            response = \"\".join(output)\n",
    "        else:\n",
    "            response = str(output)\n",
    "            \n",
    "        print(f\"[OK] Connection successful with {model_name}\")\n",
    "        print(f\"Test response: {response.strip()}\")\n",
    "        return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Connection error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Test connection\n",
    "api_ready = test_replicate_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67c98dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to get response from GPT-5-mini\n",
    "def get_response(question):\n",
    "    try:\n",
    "        output = replicate.run(\n",
    "            model_name,\n",
    "            input={\n",
    "                \"prompt\": question,\n",
    "                \"max_tokens\": 512,\n",
    "                \"temperature\": 0.2\n",
    "            }\n",
    "        )\n",
    "        if isinstance(output, list):\n",
    "            return ''.join(output).strip()\n",
    "        return str(output).strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "834f2e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with a medical question:\n",
      "Question: What are the common symptoms of type 2 diabetes?\n",
      "Response: Common symptoms of type 2 diabetes (which can develop gradually and may be mild or overlooked at first) include:\n",
      "\n",
      "- Increased thirst and frequent urination (polyuria)\n",
      "- Increased hunger (polyphagia)\n",
      "- Unexplained weight loss or sometimes weight gain\n",
      "- Fatigue or feeling unusually tired\n",
      "- Blurred vision\n",
      "- Slow-healing cuts or wounds\n",
      "- Frequent infections (e.g., skin, urinary tract)\n",
      "- Tingling, numbness, or pain in the hands or feet (peripheral neuropathy)\n",
      "- Dry, itchy skin\n",
      "- Darkened skin patches, often in body folds (acanthosis nigricans)\n",
      "\n",
      "Because type 2 diabetes can be asymptomatic for a long time, many people are diagnosed only after routine blood tests or when complications begin. If you or someone else has several of these symptoms, especially persistent thirst, frequent urination, unexplained weight change, or slow healing, see a healthcare provider for evaluation and blood glucose testing.\n",
      "[OK] Test passed - ready for evaluation!\n",
      "Question: What are the common symptoms of type 2 diabetes?\n",
      "Response: Common symptoms of type 2 diabetes (which can develop gradually and may be mild or overlooked at first) include:\n",
      "\n",
      "- Increased thirst and frequent urination (polyuria)\n",
      "- Increased hunger (polyphagia)\n",
      "- Unexplained weight loss or sometimes weight gain\n",
      "- Fatigue or feeling unusually tired\n",
      "- Blurred vision\n",
      "- Slow-healing cuts or wounds\n",
      "- Frequent infections (e.g., skin, urinary tract)\n",
      "- Tingling, numbness, or pain in the hands or feet (peripheral neuropathy)\n",
      "- Dry, itchy skin\n",
      "- Darkened skin patches, often in body folds (acanthosis nigricans)\n",
      "\n",
      "Because type 2 diabetes can be asymptomatic for a long time, many people are diagnosed only after routine blood tests or when complications begin. If you or someone else has several of these symptoms, especially persistent thirst, frequent urination, unexplained weight change, or slow healing, see a healthcare provider for evaluation and blood glucose testing.\n",
      "[OK] Test passed - ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample question\n",
    "if api_ready:\n",
    "    print(\"Testing with a medical question:\")\n",
    "    test_question = \"What are the common symptoms of type 2 diabetes?\"\n",
    "    test_response = get_response(test_question)\n",
    "    print(f\"Question: {test_question}\")\n",
    "    print(f\"Response: {test_response}\")\n",
    "    \n",
    "    if \"Error:\" in test_response:\n",
    "        print(\"[ERROR] Test failed - check API configuration\")\n",
    "    else:\n",
    "        print(\"[OK] Test passed - ready for evaluation!\")\n",
    "else:\n",
    "    print(\"[ERROR] API is not ready. Please check your Replicate API token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62dda936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL DATASET EVALUATION SUMMARY\n",
      "==================================================\n",
      "Total samples to process: 2996\n",
      "Model: openai/gpt-5-mini\n",
      "Output file: gpt5mini_full_evaluation.csv\n",
      "Checkpoint interval: Every 100 samples\n",
      "Estimated time: ~99.9 minutes\n",
      "==================================================\n",
      "[OK] API is ready - starting full evaluation\n"
     ]
    }
   ],
   "source": [
    "# Evaluation summary for full dataset\n",
    "print(\"FULL DATASET EVALUATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples to process: {len(df_sample)}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Output file: gpt5mini_full_evaluation.csv\")\n",
    "print(f\"Checkpoint interval: Every 100 samples\")\n",
    "print(f\"Estimated time: ~{(len(df_sample) * 2) / 60:.1f} minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if api_ready:\n",
    "    print(\"[OK] API is ready - starting full evaluation\")\n",
    "else:\n",
    "    print(\"[ERROR] API not ready. Please check your setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5274154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of 2996 samples using openai/gpt-5-mini...\n",
      "Checkpoints will be saved every 100 samples\n",
      "------------------------------------------------------------\n",
      "Progress: 0/2996 (0.0%) | Rate: 0.0/hour | ETA: 0.0h\n",
      "Failed requests: 0\n",
      "Progress: 50/2996 (1.7%) | Rate: 233.0/hour | ETA: 12.6h\n",
      "Failed requests: 0\n",
      "Progress: 50/2996 (1.7%) | Rate: 233.0/hour | ETA: 12.6h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 100 results to checkpoint_100.csv\n",
      "[CHECKPOINT] Saved 100 results to checkpoint_100.csv\n",
      "Progress: 100/2996 (3.3%) | Rate: 226.4/hour | ETA: 12.8h\n",
      "Failed requests: 0\n",
      "Progress: 100/2996 (3.3%) | Rate: 226.4/hour | ETA: 12.8h\n",
      "Failed requests: 0\n",
      "Progress: 150/2996 (5.0%) | Rate: 225.6/hour | ETA: 12.6h\n",
      "Failed requests: 0\n",
      "Progress: 150/2996 (5.0%) | Rate: 225.6/hour | ETA: 12.6h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 200 results to checkpoint_200.csv\n",
      "[CHECKPOINT] Saved 200 results to checkpoint_200.csv\n",
      "Progress: 200/2996 (6.7%) | Rate: 216.9/hour | ETA: 12.9h\n",
      "Failed requests: 0\n",
      "Progress: 200/2996 (6.7%) | Rate: 216.9/hour | ETA: 12.9h\n",
      "Failed requests: 0\n",
      "Progress: 250/2996 (8.3%) | Rate: 214.5/hour | ETA: 12.8h\n",
      "Failed requests: 0\n",
      "Progress: 250/2996 (8.3%) | Rate: 214.5/hour | ETA: 12.8h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 300 results to checkpoint_300.csv\n",
      "[CHECKPOINT] Saved 300 results to checkpoint_300.csv\n",
      "Progress: 300/2996 (10.0%) | Rate: 209.4/hour | ETA: 12.9h\n",
      "Failed requests: 0\n",
      "Progress: 300/2996 (10.0%) | Rate: 209.4/hour | ETA: 12.9h\n",
      "Failed requests: 0\n",
      "Progress: 350/2996 (11.7%) | Rate: 213.4/hour | ETA: 12.4h\n",
      "Failed requests: 0\n",
      "Progress: 350/2996 (11.7%) | Rate: 213.4/hour | ETA: 12.4h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 400 results to checkpoint_400.csv\n",
      "[CHECKPOINT] Saved 400 results to checkpoint_400.csv\n",
      "Progress: 400/2996 (13.4%) | Rate: 215.0/hour | ETA: 12.1h\n",
      "Failed requests: 0\n",
      "Progress: 400/2996 (13.4%) | Rate: 215.0/hour | ETA: 12.1h\n",
      "Failed requests: 0\n",
      "Progress: 450/2996 (15.0%) | Rate: 217.2/hour | ETA: 11.7h\n",
      "Failed requests: 0\n",
      "Progress: 450/2996 (15.0%) | Rate: 217.2/hour | ETA: 11.7h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 500 results to checkpoint_500.csv\n",
      "[CHECKPOINT] Saved 500 results to checkpoint_500.csv\n",
      "Progress: 500/2996 (16.7%) | Rate: 221.2/hour | ETA: 11.3h\n",
      "Failed requests: 0\n",
      "Progress: 500/2996 (16.7%) | Rate: 221.2/hour | ETA: 11.3h\n",
      "Failed requests: 0\n",
      "Progress: 550/2996 (18.4%) | Rate: 225.0/hour | ETA: 10.9h\n",
      "Failed requests: 0\n",
      "Progress: 550/2996 (18.4%) | Rate: 225.0/hour | ETA: 10.9h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 600 results to checkpoint_600.csv\n",
      "[CHECKPOINT] Saved 600 results to checkpoint_600.csv\n",
      "Progress: 600/2996 (20.0%) | Rate: 225.5/hour | ETA: 10.6h\n",
      "Failed requests: 0\n",
      "Progress: 600/2996 (20.0%) | Rate: 225.5/hour | ETA: 10.6h\n",
      "Failed requests: 0\n",
      "Progress: 650/2996 (21.7%) | Rate: 226.8/hour | ETA: 10.3h\n",
      "Failed requests: 0\n",
      "Progress: 650/2996 (21.7%) | Rate: 226.8/hour | ETA: 10.3h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 700 results to checkpoint_700.csv\n",
      "[CHECKPOINT] Saved 700 results to checkpoint_700.csv\n",
      "Progress: 700/2996 (23.4%) | Rate: 226.3/hour | ETA: 10.1h\n",
      "Failed requests: 0\n",
      "Progress: 700/2996 (23.4%) | Rate: 226.3/hour | ETA: 10.1h\n",
      "Failed requests: 0\n",
      "Progress: 750/2996 (25.0%) | Rate: 226.0/hour | ETA: 9.9h\n",
      "Failed requests: 0\n",
      "Progress: 750/2996 (25.0%) | Rate: 226.0/hour | ETA: 9.9h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 800 results to checkpoint_800.csv\n",
      "[CHECKPOINT] Saved 800 results to checkpoint_800.csv\n",
      "Progress: 800/2996 (26.7%) | Rate: 225.9/hour | ETA: 9.7h\n",
      "Failed requests: 0\n",
      "Progress: 800/2996 (26.7%) | Rate: 225.9/hour | ETA: 9.7h\n",
      "Failed requests: 0\n",
      "Progress: 850/2996 (28.4%) | Rate: 225.4/hour | ETA: 9.5h\n",
      "Failed requests: 0\n",
      "Progress: 850/2996 (28.4%) | Rate: 225.4/hour | ETA: 9.5h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 900 results to checkpoint_900.csv\n",
      "[CHECKPOINT] Saved 900 results to checkpoint_900.csv\n",
      "Progress: 900/2996 (30.0%) | Rate: 225.0/hour | ETA: 9.3h\n",
      "Failed requests: 0\n",
      "Progress: 900/2996 (30.0%) | Rate: 225.0/hour | ETA: 9.3h\n",
      "Failed requests: 0\n",
      "Progress: 950/2996 (31.7%) | Rate: 225.1/hour | ETA: 9.1h\n",
      "Failed requests: 0\n",
      "Progress: 950/2996 (31.7%) | Rate: 225.1/hour | ETA: 9.1h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1000 results to checkpoint_1000.csv\n",
      "[CHECKPOINT] Saved 1000 results to checkpoint_1000.csv\n",
      "Progress: 1000/2996 (33.4%) | Rate: 225.5/hour | ETA: 8.9h\n",
      "Failed requests: 0\n",
      "Progress: 1000/2996 (33.4%) | Rate: 225.5/hour | ETA: 8.9h\n",
      "Failed requests: 0\n",
      "Progress: 1050/2996 (35.0%) | Rate: 226.3/hour | ETA: 8.6h\n",
      "Failed requests: 0\n",
      "Progress: 1050/2996 (35.0%) | Rate: 226.3/hour | ETA: 8.6h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1100 results to checkpoint_1100.csv\n",
      "[CHECKPOINT] Saved 1100 results to checkpoint_1100.csv\n",
      "Progress: 1100/2996 (36.7%) | Rate: 226.5/hour | ETA: 8.4h\n",
      "Failed requests: 0\n",
      "Progress: 1100/2996 (36.7%) | Rate: 226.5/hour | ETA: 8.4h\n",
      "Failed requests: 0\n",
      "Progress: 1150/2996 (38.4%) | Rate: 227.2/hour | ETA: 8.1h\n",
      "Failed requests: 0\n",
      "Progress: 1150/2996 (38.4%) | Rate: 227.2/hour | ETA: 8.1h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1200 results to checkpoint_1200.csv\n",
      "[CHECKPOINT] Saved 1200 results to checkpoint_1200.csv\n",
      "Progress: 1200/2996 (40.1%) | Rate: 226.9/hour | ETA: 7.9h\n",
      "Failed requests: 0\n",
      "Progress: 1200/2996 (40.1%) | Rate: 226.9/hour | ETA: 7.9h\n",
      "Failed requests: 0\n",
      "Progress: 1250/2996 (41.7%) | Rate: 226.9/hour | ETA: 7.7h\n",
      "Failed requests: 0\n",
      "Progress: 1250/2996 (41.7%) | Rate: 226.9/hour | ETA: 7.7h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1300 results to checkpoint_1300.csv\n",
      "[CHECKPOINT] Saved 1300 results to checkpoint_1300.csv\n",
      "Progress: 1300/2996 (43.4%) | Rate: 226.9/hour | ETA: 7.5h\n",
      "Failed requests: 0\n",
      "Progress: 1300/2996 (43.4%) | Rate: 226.9/hour | ETA: 7.5h\n",
      "Failed requests: 0\n",
      "Progress: 1350/2996 (45.1%) | Rate: 225.6/hour | ETA: 7.3h\n",
      "Failed requests: 0\n",
      "Progress: 1350/2996 (45.1%) | Rate: 225.6/hour | ETA: 7.3h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1400 results to checkpoint_1400.csv\n",
      "[CHECKPOINT] Saved 1400 results to checkpoint_1400.csv\n",
      "Progress: 1400/2996 (46.7%) | Rate: 226.0/hour | ETA: 7.1h\n",
      "Failed requests: 0\n",
      "Progress: 1400/2996 (46.7%) | Rate: 226.0/hour | ETA: 7.1h\n",
      "Failed requests: 0\n",
      "Progress: 1450/2996 (48.4%) | Rate: 226.5/hour | ETA: 6.8h\n",
      "Failed requests: 0\n",
      "Progress: 1450/2996 (48.4%) | Rate: 226.5/hour | ETA: 6.8h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1500 results to checkpoint_1500.csv\n",
      "[CHECKPOINT] Saved 1500 results to checkpoint_1500.csv\n",
      "Progress: 1500/2996 (50.1%) | Rate: 226.8/hour | ETA: 6.6h\n",
      "Failed requests: 0\n",
      "Progress: 1500/2996 (50.1%) | Rate: 226.8/hour | ETA: 6.6h\n",
      "Failed requests: 0\n",
      "Progress: 1550/2996 (51.7%) | Rate: 227.4/hour | ETA: 6.4h\n",
      "Failed requests: 0\n",
      "Progress: 1550/2996 (51.7%) | Rate: 227.4/hour | ETA: 6.4h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1600 results to checkpoint_1600.csv\n",
      "[CHECKPOINT] Saved 1600 results to checkpoint_1600.csv\n",
      "Progress: 1600/2996 (53.4%) | Rate: 227.7/hour | ETA: 6.1h\n",
      "Failed requests: 0\n",
      "Progress: 1600/2996 (53.4%) | Rate: 227.7/hour | ETA: 6.1h\n",
      "Failed requests: 0\n",
      "Progress: 1650/2996 (55.1%) | Rate: 227.6/hour | ETA: 5.9h\n",
      "Failed requests: 0\n",
      "Progress: 1650/2996 (55.1%) | Rate: 227.6/hour | ETA: 5.9h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1700 results to checkpoint_1700.csv\n",
      "[CHECKPOINT] Saved 1700 results to checkpoint_1700.csv\n",
      "Progress: 1700/2996 (56.7%) | Rate: 227.7/hour | ETA: 5.7h\n",
      "Failed requests: 0\n",
      "Progress: 1700/2996 (56.7%) | Rate: 227.7/hour | ETA: 5.7h\n",
      "Failed requests: 0\n",
      "Progress: 1750/2996 (58.4%) | Rate: 228.0/hour | ETA: 5.5h\n",
      "Failed requests: 0\n",
      "Progress: 1750/2996 (58.4%) | Rate: 228.0/hour | ETA: 5.5h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1800 results to checkpoint_1800.csv\n",
      "[CHECKPOINT] Saved 1800 results to checkpoint_1800.csv\n",
      "Progress: 1800/2996 (60.1%) | Rate: 228.2/hour | ETA: 5.2h\n",
      "Failed requests: 0\n",
      "Progress: 1800/2996 (60.1%) | Rate: 228.2/hour | ETA: 5.2h\n",
      "Failed requests: 0\n",
      "Progress: 1850/2996 (61.7%) | Rate: 228.2/hour | ETA: 5.0h\n",
      "Failed requests: 0\n",
      "Progress: 1850/2996 (61.7%) | Rate: 228.2/hour | ETA: 5.0h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 1900 results to checkpoint_1900.csv\n",
      "[CHECKPOINT] Saved 1900 results to checkpoint_1900.csv\n",
      "Progress: 1900/2996 (63.4%) | Rate: 228.1/hour | ETA: 4.8h\n",
      "Failed requests: 0\n",
      "Progress: 1900/2996 (63.4%) | Rate: 228.1/hour | ETA: 4.8h\n",
      "Failed requests: 0\n",
      "Progress: 1950/2996 (65.1%) | Rate: 228.0/hour | ETA: 4.6h\n",
      "Failed requests: 0\n",
      "Progress: 1950/2996 (65.1%) | Rate: 228.0/hour | ETA: 4.6h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2000 results to checkpoint_2000.csv\n",
      "[CHECKPOINT] Saved 2000 results to checkpoint_2000.csv\n",
      "Progress: 2000/2996 (66.8%) | Rate: 228.5/hour | ETA: 4.4h\n",
      "Failed requests: 0\n",
      "Progress: 2000/2996 (66.8%) | Rate: 228.5/hour | ETA: 4.4h\n",
      "Failed requests: 0\n",
      "Progress: 2050/2996 (68.4%) | Rate: 228.9/hour | ETA: 4.1h\n",
      "Failed requests: 0\n",
      "Progress: 2050/2996 (68.4%) | Rate: 228.9/hour | ETA: 4.1h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2100 results to checkpoint_2100.csv\n",
      "[CHECKPOINT] Saved 2100 results to checkpoint_2100.csv\n",
      "Progress: 2100/2996 (70.1%) | Rate: 229.9/hour | ETA: 3.9h\n",
      "Failed requests: 0\n",
      "Progress: 2100/2996 (70.1%) | Rate: 229.9/hour | ETA: 3.9h\n",
      "Failed requests: 0\n",
      "Progress: 2150/2996 (71.8%) | Rate: 230.2/hour | ETA: 3.7h\n",
      "Failed requests: 0\n",
      "Progress: 2150/2996 (71.8%) | Rate: 230.2/hour | ETA: 3.7h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2200 results to checkpoint_2200.csv\n",
      "[CHECKPOINT] Saved 2200 results to checkpoint_2200.csv\n",
      "Progress: 2200/2996 (73.4%) | Rate: 229.8/hour | ETA: 3.5h\n",
      "Failed requests: 0\n",
      "Progress: 2200/2996 (73.4%) | Rate: 229.8/hour | ETA: 3.5h\n",
      "Failed requests: 0\n",
      "Progress: 2250/2996 (75.1%) | Rate: 230.0/hour | ETA: 3.2h\n",
      "Failed requests: 0\n",
      "Progress: 2250/2996 (75.1%) | Rate: 230.0/hour | ETA: 3.2h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2300 results to checkpoint_2300.csv\n",
      "[CHECKPOINT] Saved 2300 results to checkpoint_2300.csv\n",
      "Progress: 2300/2996 (76.8%) | Rate: 230.4/hour | ETA: 3.0h\n",
      "Failed requests: 0\n",
      "Progress: 2300/2996 (76.8%) | Rate: 230.4/hour | ETA: 3.0h\n",
      "Failed requests: 0\n",
      "Progress: 2350/2996 (78.4%) | Rate: 230.2/hour | ETA: 2.8h\n",
      "Failed requests: 0\n",
      "Progress: 2350/2996 (78.4%) | Rate: 230.2/hour | ETA: 2.8h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2400 results to checkpoint_2400.csv\n",
      "[CHECKPOINT] Saved 2400 results to checkpoint_2400.csv\n",
      "Progress: 2400/2996 (80.1%) | Rate: 230.3/hour | ETA: 2.6h\n",
      "Failed requests: 0\n",
      "Progress: 2400/2996 (80.1%) | Rate: 230.3/hour | ETA: 2.6h\n",
      "Failed requests: 0\n",
      "Progress: 2450/2996 (81.8%) | Rate: 230.1/hour | ETA: 2.4h\n",
      "Failed requests: 0\n",
      "Progress: 2450/2996 (81.8%) | Rate: 230.1/hour | ETA: 2.4h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2500 results to checkpoint_2500.csv\n",
      "[CHECKPOINT] Saved 2500 results to checkpoint_2500.csv\n",
      "Progress: 2500/2996 (83.4%) | Rate: 230.0/hour | ETA: 2.2h\n",
      "Failed requests: 0\n",
      "Progress: 2500/2996 (83.4%) | Rate: 230.0/hour | ETA: 2.2h\n",
      "Failed requests: 0\n",
      "Progress: 2550/2996 (85.1%) | Rate: 229.6/hour | ETA: 1.9h\n",
      "Failed requests: 0\n",
      "Progress: 2550/2996 (85.1%) | Rate: 229.6/hour | ETA: 1.9h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2600 results to checkpoint_2600.csv\n",
      "[CHECKPOINT] Saved 2600 results to checkpoint_2600.csv\n",
      "Progress: 2600/2996 (86.8%) | Rate: 229.7/hour | ETA: 1.7h\n",
      "Failed requests: 0\n",
      "Progress: 2600/2996 (86.8%) | Rate: 229.7/hour | ETA: 1.7h\n",
      "Failed requests: 0\n",
      "Progress: 2650/2996 (88.5%) | Rate: 229.7/hour | ETA: 1.5h\n",
      "Failed requests: 0\n",
      "Progress: 2650/2996 (88.5%) | Rate: 229.7/hour | ETA: 1.5h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2700 results to checkpoint_2700.csv\n",
      "[CHECKPOINT] Saved 2700 results to checkpoint_2700.csv\n",
      "Progress: 2700/2996 (90.1%) | Rate: 229.8/hour | ETA: 1.3h\n",
      "Failed requests: 0\n",
      "Progress: 2700/2996 (90.1%) | Rate: 229.8/hour | ETA: 1.3h\n",
      "Failed requests: 0\n",
      "Progress: 2750/2996 (91.8%) | Rate: 229.5/hour | ETA: 1.1h\n",
      "Failed requests: 0\n",
      "Progress: 2750/2996 (91.8%) | Rate: 229.5/hour | ETA: 1.1h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2800 results to checkpoint_2800.csv\n",
      "[CHECKPOINT] Saved 2800 results to checkpoint_2800.csv\n",
      "Progress: 2800/2996 (93.5%) | Rate: 229.3/hour | ETA: 0.9h\n",
      "Failed requests: 0\n",
      "Progress: 2800/2996 (93.5%) | Rate: 229.3/hour | ETA: 0.9h\n",
      "Failed requests: 0\n",
      "Progress: 2850/2996 (95.1%) | Rate: 229.6/hour | ETA: 0.6h\n",
      "Failed requests: 0\n",
      "Progress: 2850/2996 (95.1%) | Rate: 229.6/hour | ETA: 0.6h\n",
      "Failed requests: 0\n",
      "[CHECKPOINT] Saved 2900 results to checkpoint_2900.csv\n",
      "[CHECKPOINT] Saved 2900 results to checkpoint_2900.csv\n",
      "Progress: 2900/2996 (96.8%) | Rate: 229.9/hour | ETA: 0.4h\n",
      "Failed requests: 0\n",
      "Progress: 2900/2996 (96.8%) | Rate: 229.9/hour | ETA: 0.4h\n",
      "Failed requests: 0\n",
      "Progress: 2950/2996 (98.5%) | Rate: 230.2/hour | ETA: 0.2h\n",
      "Failed requests: 0\n",
      "Progress: 2950/2996 (98.5%) | Rate: 230.2/hour | ETA: 0.2h\n",
      "Failed requests: 0\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETED!\n",
      "Successfully processed: 2996 samples\n",
      "Failed requests: 0\n",
      "Total time: 13.01 hours\n",
      "Results saved to 'gpt5mini_full_evaluation.csv'\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETED!\n",
      "Successfully processed: 2996 samples\n",
      "Failed requests: 0\n",
      "Total time: 13.01 hours\n",
      "Results saved to 'gpt5mini_full_evaluation.csv'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on full dataset with checkpoints\n",
    "import time\n",
    "\n",
    "results = []\n",
    "start_time = time.time()\n",
    "total_samples = len(df_sample)\n",
    "failed_requests = 0\n",
    "\n",
    "print(f\"Starting evaluation of {total_samples} samples using {model_name}...\")\n",
    "print(f\"Checkpoints will be saved every 100 samples\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, row in df_sample.iterrows():\n",
    "    # Progress updates every 50 samples\n",
    "    if i % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i / elapsed * 3600) if elapsed > 0 else 0\n",
    "        eta_hours = (total_samples - i) / rate if rate > 0 else 0\n",
    "        completed_pct = (i / total_samples) * 100\n",
    "        print(f\"Progress: {i}/{total_samples} ({completed_pct:.1f}%) | Rate: {rate:.1f}/hour | ETA: {eta_hours:.1f}h\")\n",
    "        print(f\"Failed requests: {failed_requests}\")\n",
    "    \n",
    "    question = row['question']\n",
    "    reference = row['reference'] if 'reference' in row else ''\n",
    "    \n",
    "    # Get model response\n",
    "    generated = get_response(question)\n",
    "    \n",
    "    # Check for errors\n",
    "    if \"Error:\" in str(generated):\n",
    "        failed_requests += 1\n",
    "        print(f\"[WARNING] Failed request at sample {i}\")\n",
    "        continue\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'question': question,\n",
    "        'reference': reference,\n",
    "        'generated': generated\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint every 100 samples\n",
    "    if (i + 1) % 100 == 0:\n",
    "        temp_df = pd.DataFrame(results)\n",
    "        checkpoint_file = f'checkpoint_{i+1}.csv'\n",
    "        temp_df.to_csv(checkpoint_file, index=False)\n",
    "        print(f\"[CHECKPOINT] Saved {len(results)} results to {checkpoint_file}\")\n",
    "    \n",
    "    # Rate limiting - 1 second delay to avoid hitting API limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Final save\n",
    "if len(results) > 0:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('gpt5mini_full_evaluation.csv', index=False)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION COMPLETED!\")\n",
    "    print(f\"Successfully processed: {len(results_df)} samples\")\n",
    "    print(f\"Failed requests: {failed_requests}\")\n",
    "    print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "    print(f\"Results saved to 'gpt5mini_full_evaluation.csv'\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\n[ERROR] NO VALID RESULTS - Evaluation failed\")\n",
    "    print(\"Please check the model configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97808ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL EVALUATION RESULTS ===\n",
      "Total samples evaluated: 2996\n",
      "Model: openai/gpt-5-mini\n",
      "\n",
      "Sample results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "Question: Hi doctor,\n",
      "I am a 51-year-old female with a height of 5 feet 8 inches and a weight of 145 lbs. I hav...\n",
      "Generated: Thank you — that sounds very frightening. I can’t make a diagnosis or replace your doctor, but I can...\n",
      "\n",
      "--- Sample 2 ---\n",
      "Question: Hello doctor,\n",
      "I am 29 years old.\n",
      "CT scan came up negative for problems associated with infection. I ...\n",
      "Generated: Thank you — I can help review this and give a general, non‑diagnostic second opinion based on the in...\n",
      "\n",
      "--- Sample 3 ---\n",
      "Question: Hi doctor,\n",
      "For the past two days, I am suffering from a cold because of climatic change. I am having...\n",
      "Generated: Thanks — sorry you’re feeling unwell. A few quick questions to make sure I give safe, useful advice:...\n",
      "\n",
      "--- Sample 4 ---\n",
      "Question: Hi doctor,\n",
      "I am suffering from kidney stones problem for the past four days.  I took an ultrasound s...\n",
      "Generated: I’m sorry you’re in pain — that sounds uncomfortable. I can’t make a diagnosis over chat, but I can ...\n",
      "\n",
      "--- Sample 5 ---\n",
      "Question: Hi doctor,\n",
      "I am a 20-year-old female. I am having early morning blood in saliva for the past two mon...\n",
      "Generated: Thanks — useful details. A few quick follow-up questions, then a likely explanation and practical ne...\n",
      "\n",
      "Full results saved to 'gpt5mini_full_evaluation.csv'\n",
      "Dataset size: 2996 samples\n",
      "Average response length: 4140.9 characters\n"
     ]
    }
   ],
   "source": [
    "# Display full evaluation results\n",
    "try:\n",
    "    results_df = pd.read_csv('gpt5mini_full_evaluation.csv')\n",
    "    \n",
    "    print(\"=== FULL EVALUATION RESULTS ===\")\n",
    "    print(f\"Total samples evaluated: {len(results_df)}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    print(\"\\nSample results:\")\n",
    "    for i in range(min(5, len(results_df))):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"Question: {results_df.iloc[i]['question'][:100]}...\")\n",
    "        print(f\"Generated: {results_df.iloc[i]['generated'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nFull results saved to 'gpt5mini_full_evaluation.csv'\")\n",
    "    print(f\"Dataset size: {len(results_df)} samples\")\n",
    "    \n",
    "    # Show statistics\n",
    "    avg_response_length = results_df['generated'].str.len().mean()\n",
    "    print(f\"Average response length: {avg_response_length:.1f} characters\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"[ERROR] No evaluation results found. Please run the evaluation first.\")\n",
    "    print(\"Looking for: gpt5mini_full_evaluation.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error loading results: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e82bbcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import evaluation metrics libraries\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Setup metrics\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smoothing = SmoothingFunction().method1\n",
    "\n",
    "print(\"Evaluation metrics libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f01f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics calculation function ready\n"
     ]
    }
   ],
   "source": [
    "# Define metrics calculation function\n",
    "def calculate_metrics(reference, generated):\n",
    "    \"\"\"Calculate BLEU and ROUGE metrics for a reference-generated pair\"\"\"\n",
    "    try:\n",
    "        # Tokenize for BLEU calculation\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        gen_tokens = nltk.word_tokenize(generated.lower())\n",
    "        \n",
    "        # Calculate BLEU scores\n",
    "        bleu1 = sentence_bleu([ref_tokens], gen_tokens, weights=(1,0,0,0), smoothing_function=smoothing)\n",
    "        bleu4 = sentence_bleu([ref_tokens], gen_tokens, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothing)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer_obj.score(reference, generated)\n",
    "        \n",
    "        return {\n",
    "            'bleu1': bleu1,\n",
    "            'bleu4': bleu4,\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics for sample: {e}\")\n",
    "        return {\n",
    "            'bleu1': 0.0,\n",
    "            'bleu4': 0.0,\n",
    "            'rouge1': 0.0,\n",
    "            'rouge2': 0.0,\n",
    "            'rougeL': 0.0\n",
    "        }\n",
    "\n",
    "print(\"Metrics calculation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e2c199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded evaluation results with 2996 samples\n",
      "Calculating metrics for each sample...\n",
      "Processing sample 0/2996\n",
      "Processing sample 100/2996\n",
      "Processing sample 200/2996\n",
      "Processing sample 300/2996\n",
      "Processing sample 400/2996\n",
      "Processing sample 500/2996\n",
      "Processing sample 600/2996\n",
      "Processing sample 700/2996\n",
      "Processing sample 800/2996\n",
      "Processing sample 900/2996\n",
      "Processing sample 1000/2996\n",
      "Processing sample 1100/2996\n",
      "Processing sample 1200/2996\n",
      "Processing sample 1300/2996\n",
      "Processing sample 1400/2996\n",
      "Processing sample 1500/2996\n",
      "Processing sample 1600/2996\n",
      "Processing sample 1700/2996\n",
      "Processing sample 1800/2996\n",
      "Processing sample 1900/2996\n",
      "Processing sample 2000/2996\n",
      "Processing sample 2100/2996\n",
      "Processing sample 2200/2996\n",
      "Processing sample 2300/2996\n",
      "Processing sample 2400/2996\n",
      "Processing sample 2500/2996\n",
      "Processing sample 2600/2996\n",
      "Processing sample 2700/2996\n",
      "Processing sample 2800/2996\n",
      "Processing sample 2900/2996\n",
      "Error calculating metrics: [Errno 13] Permission denied: 'gpt5mini_full_evaluation_with_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for each sample in the evaluation results\n",
    "try:\n",
    "    # Load the evaluation results\n",
    "    results_df = pd.read_csv('gpt5mini_full_evaluation.csv')\n",
    "    print(f\"Loaded evaluation results with {len(results_df)} samples\")\n",
    "    \n",
    "    # Check if metrics columns already exist\n",
    "    metric_columns = ['bleu1', 'bleu4', 'rouge1', 'rouge2', 'rougeL']\n",
    "    missing_metrics = [col for col in metric_columns if col not in results_df.columns]\n",
    "    \n",
    "    if missing_metrics:\n",
    "        print(\"Calculating metrics for each sample...\")\n",
    "        \n",
    "        # Add metrics columns\n",
    "        for metric in metric_columns:\n",
    "            results_df[metric] = None\n",
    "        \n",
    "        # Calculate metrics for each sample\n",
    "        valid_metrics_count = 0\n",
    "        for i, row in results_df.iterrows():\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processing sample {i}/{len(results_df)}\")\n",
    "            \n",
    "            reference = row['reference'] if pd.notna(row['reference']) else ''\n",
    "            generated = row['generated'] if pd.notna(row['generated']) else ''\n",
    "            \n",
    "            # Only calculate metrics if both reference and generated text exist\n",
    "            if reference and reference.strip() and generated and generated.strip():\n",
    "                metrics = calculate_metrics(reference, generated)\n",
    "                for metric, value in metrics.items():\n",
    "                    results_df.at[i, metric] = value\n",
    "                valid_metrics_count += 1\n",
    "        \n",
    "        # Save updated results with metrics\n",
    "        results_df.to_csv('gpt5mini_full_evaluation_with_metrics.csv', index=False)\n",
    "        print(f\"\\nMetrics calculated for {valid_metrics_count} samples\")\n",
    "        print(\"Updated results saved to 'gpt5mini_full_evaluation_with_metrics.csv'\")\n",
    "    else:\n",
    "        print(\"Metrics columns already exist in the dataset\")\n",
    "        valid_metrics_count = results_df['bleu1'].notna().sum()\n",
    "        print(f\"Found metrics for {valid_metrics_count} samples\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No evaluation results found.\")\n",
    "    print(\"Please run the evaluation first to generate 'gpt5mini_full_evaluation.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating metrics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07763786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded results with calculated metrics\n",
      "\n",
      "============================================================\n",
      "EVALUATION METRICS ANALYSIS\n",
      "============================================================\n",
      "Total samples: 2996\n",
      "Model: openai/gpt-5-mini\n",
      "\n",
      "=== AVERAGE METRICS (5 metrics available) ===\n",
      "    BLEU-1: 0.1235 (±0.0622) | Range: [0.0064, 0.4579] | 2996 samples\n",
      "    BLEU-4: 0.0065 (±0.0064) | Range: [0.0003, 0.0874] | 2996 samples\n",
      "   ROUGE-1: 0.2024 (±0.0768) | Range: [0.0085, 0.4770] | 2996 samples\n",
      "   ROUGE-2: 0.0290 (±0.0171) | Range: [0.0000, 0.1673] | 2996 samples\n",
      "   ROUGE-L: 0.0914 (±0.0265) | Range: [0.0085, 0.2101] | 2996 samples\n",
      "\n",
      "=== SCORE DISTRIBUTION (QUARTILES) ===\n",
      "    BLEU-1: Q25=0.0803 | Median=0.1126 | Q75=0.1553\n",
      "    BLEU-4: Q25=0.0026 | Median=0.0046 | Q75=0.0078\n",
      "   ROUGE-1: Q25=0.1486 | Median=0.1985 | Q75=0.2517\n",
      "   ROUGE-2: Q25=0.0170 | Median=0.0260 | Q75=0.0379\n",
      "   ROUGE-L: Q25=0.0739 | Median=0.0910 | Q75=0.1091\n",
      "\n",
      "=== SAMPLE ANALYSIS ===\n",
      "\n",
      "Best performing sample (highest BLEU-1: 0.4579):\n",
      "Question: Hi,\n",
      "I am a 31 year old male. About 38 days before I had sex with a CSW Commercial Social Worker. It ...\n",
      "Reference: Hi,\n",
      "Welcome to icliniq.com.\n",
      "Based upon your history, my answer to your questions\n",
      "1. Are My Symptoms ...\n",
      "Generated: Thanks — I’ll address each question directly and clearly. Nothing below is a substitute for a person...\n",
      "\n",
      "Worst performing sample (lowest BLEU-1: 0.0064):\n",
      "Question: Hello doctor,\n",
      "I was infected with a fungal infection jock itch when I was 14 years old, and I am 36 ...\n",
      "Reference: Hi,\n",
      "Welcome to icliniq.com.\n",
      "Please send few good quality photographs taken under natural light....\n",
      "Generated: Thank you — that sounds miserable. I can’t diagnose you over messages, but I can explain likely caus...\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Samples with calculated metrics: 2996\n",
      "Samples with reference text: 2996\n",
      "Average response length: 4140.9 characters\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display average metrics\n",
    "try:\n",
    "    # Load results with metrics\n",
    "    try:\n",
    "        results_df = pd.read_csv('gpt5mini_full_evaluation_with_metrics.csv')\n",
    "        print(\"Loaded results with calculated metrics\")\n",
    "    except FileNotFoundError:\n",
    "        results_df = pd.read_csv('gpt5mini_full_evaluation.csv')\n",
    "        print(\"Loaded original results (metrics may not be available)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION METRICS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total samples: {len(results_df)}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    # Check which metrics are available\n",
    "    metric_columns = ['bleu1', 'bleu4', 'rouge1', 'rouge2', 'rougeL']\n",
    "    metric_names = ['BLEU-1', 'BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "    \n",
    "    available_metrics = [col for col in metric_columns if col in results_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        print(f\"\\n=== AVERAGE METRICS ({len(available_metrics)} metrics available) ===\")\n",
    "        \n",
    "        metrics_summary = {}\n",
    "        for metric, name in zip(metric_columns, metric_names):\n",
    "            if metric in results_df.columns:\n",
    "                # Filter out None/NaN values\n",
    "                valid_scores = results_df[metric].dropna()\n",
    "                if len(valid_scores) > 0:\n",
    "                    avg_score = valid_scores.mean()\n",
    "                    std_score = valid_scores.std()\n",
    "                    min_score = valid_scores.min()\n",
    "                    max_score = valid_scores.max()\n",
    "                    \n",
    "                    metrics_summary[metric] = {\n",
    "                        'avg': avg_score, \n",
    "                        'std': std_score, \n",
    "                        'min': min_score, \n",
    "                        'max': max_score,\n",
    "                        'count': len(valid_scores)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"{name:>10}: {avg_score:.4f} (±{std_score:.4f}) | Range: [{min_score:.4f}, {max_score:.4f}] | {len(valid_scores)} samples\")\n",
    "                else:\n",
    "                    print(f\"{name:>10}: No valid scores\")\n",
    "        \n",
    "        # Show quartile distribution\n",
    "        if metrics_summary:\n",
    "            print(f\"\\n=== SCORE DISTRIBUTION (QUARTILES) ===\")\n",
    "            for metric, name in zip(metric_columns, metric_names):\n",
    "                if metric in metrics_summary:\n",
    "                    valid_scores = results_df[metric].dropna()\n",
    "                    q25 = valid_scores.quantile(0.25)\n",
    "                    q50 = valid_scores.quantile(0.50)  # median\n",
    "                    q75 = valid_scores.quantile(0.75)\n",
    "                    print(f\"{name:>10}: Q25={q25:.4f} | Median={q50:.4f} | Q75={q75:.4f}\")\n",
    "        \n",
    "        # Show best and worst performing samples\n",
    "        if 'bleu1' in results_df.columns:\n",
    "            print(f\"\\n=== SAMPLE ANALYSIS ===\")\n",
    "            valid_samples = results_df.dropna(subset=['bleu1'])\n",
    "            \n",
    "            if len(valid_samples) > 0:\n",
    "                # Best performing sample (highest BLEU-1)\n",
    "                best_idx = valid_samples['bleu1'].idxmax()\n",
    "                best_sample = results_df.loc[best_idx]\n",
    "                print(f\"\\nBest performing sample (highest BLEU-1: {best_sample['bleu1']:.4f}):\")\n",
    "                print(f\"Question: {best_sample['question'][:100]}...\")\n",
    "                print(f\"Reference: {best_sample['reference'][:100]}...\")\n",
    "                print(f\"Generated: {best_sample['generated'][:100]}...\")\n",
    "                \n",
    "                # Worst performing sample (lowest BLEU-1)\n",
    "                worst_idx = valid_samples['bleu1'].idxmin()\n",
    "                worst_sample = results_df.loc[worst_idx]\n",
    "                print(f\"\\nWorst performing sample (lowest BLEU-1: {worst_sample['bleu1']:.4f}):\")\n",
    "                print(f\"Question: {worst_sample['question'][:100]}...\")\n",
    "                print(f\"Reference: {worst_sample['reference'][:100]}...\")\n",
    "                print(f\"Generated: {worst_sample['generated'][:100]}...\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n=== SUMMARY STATISTICS ===\")\n",
    "        samples_with_metrics = results_df[available_metrics[0]].notna().sum()\n",
    "        samples_with_reference = results_df['reference'].notna().sum() if 'reference' in results_df.columns else 0\n",
    "        avg_response_length = results_df['generated'].str.len().mean()\n",
    "        \n",
    "        print(f\"Samples with calculated metrics: {samples_with_metrics}\")\n",
    "        print(f\"Samples with reference text: {samples_with_reference}\")\n",
    "        print(f\"Average response length: {avg_response_length:.1f} characters\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nNo metrics available. Please run the metrics calculation first.\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No evaluation results found.\")\n",
    "    print(\"Please run the evaluation and metrics calculation first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing metrics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e4d6b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating metrics summary report...\n",
      "Metrics summary exported to 'gpt5mini_metrics_summary.csv'\n",
      "\n",
      "METRICS SUMMARY TABLE:\n",
      "--------------------------------------------------------------------------------\n",
      "   BLEU1: 0.1235 ± 0.0622 (range: 0.0064-0.4579) | 2996 samples\n",
      "   BLEU4: 0.0065 ± 0.0064 (range: 0.0003-0.0874) | 2996 samples\n",
      "  ROUGE1: 0.2024 ± 0.0768 (range: 0.0085-0.4770) | 2996 samples\n",
      "  ROUGE2: 0.0290 ± 0.0171 (range: 0.0000-0.1673) | 2996 samples\n",
      "  ROUGEL: 0.0914 ± 0.0265 (range: 0.0085-0.2101) | 2996 samples\n",
      "\n",
      "OVERALL PERFORMANCE SCORE: 0.0905\n",
      "INTERPRETATION: NEEDS IMPROVEMENT\n",
      "\n",
      "Files created:\n",
      "- gpt5mini_full_evaluation_with_metrics.csv (detailed results)\n",
      "- gpt5mini_metrics_summary.csv (summary statistics)\n"
     ]
    }
   ],
   "source": [
    "# Export metrics summary to CSV\n",
    "try:\n",
    "    # Load results with metrics\n",
    "    try:\n",
    "        results_df = pd.read_csv('gpt5mini_full_evaluation_with_metrics.csv')\n",
    "    except FileNotFoundError:\n",
    "        results_df = pd.read_csv('gpt5mini_full_evaluation.csv')\n",
    "    \n",
    "    # Create comprehensive metrics summary\n",
    "    metric_columns = ['bleu1', 'bleu4', 'rouge1', 'rouge2', 'rougeL']\n",
    "    summary_data = []\n",
    "    \n",
    "    print(\"Creating metrics summary report...\")\n",
    "    \n",
    "    for metric in metric_columns:\n",
    "        if metric in results_df.columns:\n",
    "            valid_scores = results_df[metric].dropna()\n",
    "            if len(valid_scores) > 0:\n",
    "                summary_data.append({\n",
    "                    'metric': metric.upper(),\n",
    "                    'mean': valid_scores.mean(),\n",
    "                    'std': valid_scores.std(),\n",
    "                    'min': valid_scores.min(),\n",
    "                    'q25': valid_scores.quantile(0.25),\n",
    "                    'median': valid_scores.quantile(0.50),\n",
    "                    'q75': valid_scores.quantile(0.75),\n",
    "                    'max': valid_scores.max(),\n",
    "                    'sample_count': len(valid_scores),\n",
    "                    'model': model_name,\n",
    "                    'total_samples': len(results_df)\n",
    "                })\n",
    "    \n",
    "    if summary_data:\n",
    "        # Create and save summary DataFrame\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv('gpt5mini_metrics_summary.csv', index=False)\n",
    "        \n",
    "        print(\"Metrics summary exported to 'gpt5mini_metrics_summary.csv'\")\n",
    "        print(\"\\nMETRICS SUMMARY TABLE:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display formatted summary\n",
    "        for _, row in summary_df.iterrows():\n",
    "            print(f\"{row['metric']:>8}: {row['mean']:.4f} ± {row['std']:.4f} \"\n",
    "                  f\"(range: {row['min']:.4f}-{row['max']:.4f}) | {row['sample_count']} samples\")\n",
    "        \n",
    "        # Calculate overall performance score (weighted average)\n",
    "        if len(summary_df) >= 3:  # If we have at least 3 metrics\n",
    "            weights = {'BLEU1': 0.2, 'BLEU4': 0.2, 'ROUGE1': 0.2, 'ROUGE2': 0.2, 'ROUGEL': 0.2}\n",
    "            total_score = 0\n",
    "            total_weight = 0\n",
    "            \n",
    "            for _, row in summary_df.iterrows():\n",
    "                metric_name = row['metric']\n",
    "                if metric_name in weights:\n",
    "                    total_score += row['mean'] * weights[metric_name]\n",
    "                    total_weight += weights[metric_name]\n",
    "            \n",
    "            if total_weight > 0:\n",
    "                overall_score = total_score / total_weight\n",
    "                print(f\"\\nOVERALL PERFORMANCE SCORE: {overall_score:.4f}\")\n",
    "                \n",
    "                # Performance interpretation\n",
    "                if overall_score >= 0.4:\n",
    "                    print(\"INTERPRETATION: EXCELLENT performance\")\n",
    "                elif overall_score >= 0.3:\n",
    "                    print(\"INTERPRETATION: GOOD performance\") \n",
    "                elif overall_score >= 0.2:\n",
    "                    print(\"INTERPRETATION: MODERATE performance\")\n",
    "                else:\n",
    "                    print(\"INTERPRETATION: NEEDS IMPROVEMENT\")\n",
    "        \n",
    "        print(f\"\\nFiles created:\")\n",
    "        print(f\"- gpt5mini_full_evaluation_with_metrics.csv (detailed results)\")\n",
    "        print(f\"- gpt5mini_metrics_summary.csv (summary statistics)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No metrics data available to export\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating metrics summary: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
